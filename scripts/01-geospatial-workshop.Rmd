---
title: "Geospatial Analysis in R"
author: "Kim Kreiss"
output:
  beamer_presentation:
    slide_level: 2
    toc: true
---

# Introduction

## Why R?

-   Reproducibility and Transparency

-   Flexibility and Customization

-   Integration with Statistical Analysis

-   Geospatial Visualization

## Today

-   We will analyze crime data in Chicago together:

    -   Combine point data and boundary file
    -   Generate informative visualizations
    -   Apply statistical techniques to identify hot and cold spots

-   This is an interactive workshop, where you are provided with code and data to run locally:

    -   `01-geospatial-workshop.Rmd`
    -   `2016-2020-chicago-crime.csv`
    -   `Boundaries - Neighborhoods.geojson`
    -   `Boundaries - Census Tracts - 2010.geojson`

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=T, warning=F, message=F)
```

# Brief re-intro to spatial data and mapping

## Spatial Data

-   Most commonly point data, boundary data, grid or raster data

    -   point data: longitude and latitude of a unit of interest
    -   boundary data: polygon/shape data delineating a geographical area
    -   grid or raster data: usually squares/rectangles over the Earth's surface with specific attributes

- Uses Coordinate Reference System (CRS) 

## Mapping

-   Common application is to show point data within a region or geographic boundaries of interest, ie, are homicides in Chicago concentrated in certain neighborhoods? 


```{r}
# Load required libraries
library(sf)
library(spdep)
library(spatstat)
library(tidyverse)
library(colorspace)
```

## 
\scriptsize
```{r}
path <- here::here()
crime_data <- read.csv(paste0(path,"/data/2016-2020-chicago-crime.csv"))

neighborhoods <- st_read(
  paste0(path,
         "/data/Boundaries - Neighborhoods.geojson"),
  quiet=T)

census_tracts <- st_read(
  paste0(path,
         "/data/Boundaries - Census Tracts - 2010.geojson"), 
  quiet=T)

homicide <- crime_data %>% 
  filter(Primary.Type=="HOMICIDE")

```

# Kernel Density Estimation

# Global and Local Moran's I

# Getis-Ord / G\*

# Wrap-up

```{r}
path <- here::here()
crime_data <- read.csv(paste0(path, "/data/2016-2020-chicago-crime.csv"))

neighborhoods = st_read(paste0(path,"/data/Boundaries - Neighborhoods.geojson"), quiet=T)
#neighborhoods = st_read(paste0(path,"/data/Boundaries - Census Tracts - 2010.geojson"), quiet=T)

homicide <- crime_data %>% 
  filter(Primary.Type=="HOMICIDE")

### double check CRS and refernece 
```

```{r}
chicago_map <- neighborhoods %>%
  ggplot() + 
  geom_point(data = homicide, aes(x=Longitude, y=Latitude), shape=1, color="red",size=.005)+
  geom_sf(aes(geometry=geometry), fill="transparent", linewidth=.25) + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        rect = element_blank(), 
        legend.position = "bottom") + 
  labs(x="", y="", title = "Homicides in Chicago (2016-2019)")

chicago_map
```

```{r}
point_sf <- st_as_sf(homicide, coords = c("Longitude", "Latitude"), crs = 4326) # coordinate reference systems in slide 

neighborhoods$homicides <- lengths(st_intersects(neighborhoods, point_sf))
```

```{r}
chicago_map <- neighborhoods %>%
  ggplot() + 
  geom_sf(aes(geometry=geometry, fill=homicides), linewidth=.25) + 
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        rect = element_blank(), 
        legend.position = "bottom") + 
  labs(x="", y="", title = "Homicides in Chicago (2016-2019)") + 
  scale_fill_continuous_divergingx(palette = 'RdBu', mid = mean(neighborhoods$homicides), rev=T,name = "")

chicago_map
```

```{r}
library(spatstat)

borders = st_transform(neighborhoods$geometry, 6454)

ppp.homicide = as.ppp(st_transform(point_sf$geometry, 6454))

density.homicide = density.ppp(ppp.homicide, w=as.owin(borders), sigma=500)
density.homicide = density.ppp(ppp.homicide, sigma=500)

plot(density.homicide, col=colorRampPalette(c("navy", "gray", "red")), main = "Density of Homicides in Chicago (2016-2019)")
plot(borders, col=NA, border="gray", add=T)

## incorpoarting plotly/ double check the ggplot2
```

```{r}

# talk about different weight matrices options 
# contiguous, inverse etc., why use one over the other
# talk about how these are used, why people use in dependent, independent, etc. dependence on your question; kernel function for neighborhood weight matrix
library(spdep)

neighbors = poly2nb(as_Spatial(neighborhoods), queen=T)

weights = nb2listw(neighbors, style="W", zero.policy=T)

plot(st_geometry(neighborhoods), border="gray")

plot(neighbors, coords=st_coordinates(st_centroid(neighborhoods)), add=T)
```

```{r}
## also do global moran's I and talk about difference btwn local vs. global 

neighborhoods$rate = neighborhoods$homicides/sum(neighborhoods$homicides,na.rm = T)
local = localmoran(neighborhoods$rate, weights)

neighborhoods$Morans.2020 = local[,"Z.Ii"]

breaks=c(-10, -1, 1, 10)

plot(neighborhoods["Morans.2020"], breaks=breaks,
         pal=colorRampPalette(c("navy", "gray", "red")))
```

```{r}
neighborhoods$Getis.Ord.2020 = localG(neighborhoods$rate, weights)

plot(neighborhoods["Getis.Ord.2020"], 
	breaks=c(-10, -1.645, -1.282, 1.282, 1.645, 10),
	pal=colorRampPalette(c("navy", "#f0f0f0", "red")))
```

## Next steps:

-   descriptions/slides for each type of analysis
-   consider using different geographical boundaries, ie census tracts, highlight modifiable area unit problem + parameters for different methods
-   resources on spatial regression, future directions, etc.
-   bigger data/computation: plug for Eric's workshop!!!

package dependencies

some other demographic/neighborhood characteristics that are associated with higher crimes, etc.

add in population data for the neighborhoods and census tracts

if you have other high frequency data, ie cell phone data or all crime data as benchmark for computation time spatial heterogeneity ??

source: <https://michaelminn.net/tutorials/r-crime/>

<!-- ```{r} -->

<!-- # Load example point data -->

<!-- data <- read_csv("~/Downloads/crime.csv") %>%  -->

<!--   as.data.frame() %>% -->

<!--   filter(geo_lat!=0 & geo_lon< -5 & geo_lon > -112) -->

<!-- plot(data$geo_lon, data$geo_lat, col = "blue",  -->

<!--      xlab = "Longitude", ylab = "Latitude", main = "Point Data", size=.005, cex = .5) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- # Create crime point data -->

<!-- crime_data <- data.frame( -->

<!--   ID = 1:100, -->

<!--   Longitude = runif(100, -105.3, -104.7), # Generate random longitudes within a range -->

<!--   Latitude = runif(100, 39.6, 40.2)       # Generate random latitudes within a range -->

<!-- ) -->

<!-- coordinates(crime_data) <- c("Longitude", "Latitude")  # Convert to SpatialPointsDataFrame -->

<!-- # Create administrative polygon data (e.g., neighborhoods) -->

<!-- # For demonstration purposes, let's create a grid of polygons covering the city -->

<!-- # You would typically load administrative boundary shapefiles for real-world data -->

<!-- grid <- GridTopology(c(-105.3, 39.6), c(0.1, 0.1), c(10, 10))  # Define grid covering the city -->

<!-- polygons <- as.SpatialPolygons.GridTopology(grid)  # Convert grid to SpatialPolygons -->

<!-- # Assign attributes to the polygons -->

<!-- # For demonstration purposes, let's assign random crime counts to each polygon -->

<!-- polygons_data <- data.frame( -->

<!--   ID = 1:100, -->

<!--   Crime_Count = sample(1:20, 100, replace = TRUE)  # Random crime counts for each polygon -->

<!-- ) -->

<!-- row.names(polygons_data) <- sapply(slot(polygons, "polygons"), function(x) slot(x, "ID")) -->

<!-- # Combine the polygon data with the polygons -->

<!-- polygons <- SpatialPolygonsDataFrame(polygons, polygons_data) -->

<!-- # Plot the crime points and administrative polygons -->

<!-- plot(polygons, col = "lightblue", border = "black") -->

<!-- points(crime_data, col = "red", pch = 20) -->

<!-- # Create crime count variable in crime_data -->

<!-- crime_data$Crime_Count <- sample(1:20, nrow(crime_data), replace = TRUE) -->

<!-- ``` -->

<!-- ```{r}  -->

<!-- # Create a neighbor list based on polygon adjacency -->

<!-- W <- poly2nb(polygons) -->

<!-- W <- nb2listw(W, style = "B") -->

<!-- # Conduct Local Moran's I -->

<!-- local_moran <- localmoran(crime_data$Crime_Count, W) %>%  -->

<!--   as.data.frame() %>%  -->

<!--   mutate(ID = seq(1,100,1)) -->

<!-- # Print the results -->

<!-- print(local_moran) -->

<!-- # Conduct Getis-Ord Gi* statistics -->

<!-- getis_ord <- localG(crime_data$Crime_Count, W) %>%  -->

<!--   as.data.frame() %>%  -->

<!--   mutate(ID = seq(1,100,1)) -->

<!-- # Print the results -->

<!-- print(getis_ord) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- # Plot the administrative polygons -->

<!-- plot(polygons, col = "lightblue", border = "black") -->

<!-- # Plot Local Moran's I results as a heatmap indicating hot and cold spots -->

<!-- plot(local_moran$Ii, col = colorRampPalette(c("blue", "white", "red"))(100), add = TRUE) -->

<!-- # Add legend for Local Moran's I heatmap -->

<!-- legend("topright", legend = "Local Moran's I", fill = colorRampPalette(c("blue", "white", "red"))(100), bty = "n") -->

<!-- ``` -->
